# 对联生成模型调研

中文对联生成任务，是文本生成中一个比较常见的任务。下面基于该任务做生成模型对比调研。

#### 目的：
1. 评测[textgen](https://github.com/shibing624/textgen)库的生成模型代码完整性及可用性；
2. 评测[GPT2](https://github.com/shibing624/textgen/tree/main/textgen/language_modeling)和[T5](https://github.com/shibing624/textgen/tree/main/textgen/t5)模型在对联生成上的效果，并对比两者的差异。


## 数据集

[couplet.tar.gz](https://github.com/wb14123/couplet-dataset/releases/download/1.0/couplet.tar.gz) ，包含770491个对联。

样本：
```shell
==> .//couplet_files/couplet/train/in.txt <==
晚 风 摇 树 树 还 挺 

==> .//couplet_files/couplet/train/out.txt <==
晨 露 润 花 花 更 红 
```


## T5模型

T5模型一种Encoder-Decoder架构，其中输入和输出都是文本序列。
这使它能够灵活地执行任何自然语言处理任务，而无需以任何方式修改模型架构。这也意味着训练一个T5模型可以同时执行多个任务，只需要给它不同*prefix*即可。

关于T5模型可以参考paper：*[Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683)* 

**Tip:** 文章 [asking-the-right-questions-training-a-t5-transformer-model-on-a-new-task](https://towardsdatascience.com/asking-the-right-questions-training-a-t5-transformer-model-on-a-new-task-691ebba2d72c?source=friends_link&sk=9f88c539546eca32b702cc0243abd0dd) 介绍了训练T5模型来执行一个新任务

**Tip:** 文章 [the-guide-to-multi-tasking-with-the-t5-transformer](https://towardsdatascience.com/the-guide-to-multi-tasking-with-the-t5-transformer-90c70a08837b?source=friends_link&sk=ffe37deefa8dd4158f3f76e3dd46cf11) 介绍了训练T5模型来执行多个任务的方法

#### 指定任务
T5模型可以在开头添加*prefix*来执行某个特定任务，同一个任务使用的*prefix*相同，对于多个任务，则使用不同的*prefix*区分。

Example *prefixes*:

1. `binary classification`
2. `predict sentiment`
3. `answer question`

预测时，模型将根据不同的*prefix*来生成相应的输出。

#### 使用步骤

使用步骤：

1. 初始化 `T5Model`
2. 训练模型 `train_model()`
3. 评估模型 `eval_model()`
4. 预测 `predict()`

### 训练及预测

查看 [examples/T5/T5_Finetune_Chinese_Couplet.ipynb](https://github.com/shibing624/textgen/blob/main/examples/T5/T5_Finetune_Chinese_Couplet.ipynb) ，里面包括训练和预测。

使用全量数据训练5个epochs。

### 结果预测

|prefix|input_text|target_text|pred|
|:-- |:--- |:--- |:-- |
|对联：|书香醉我凌云梦|诗韵酬余壮志心|墨韵迷人醉月心|



## GPT2模型

GPT2模型也是从Transformer改进来的，T5同时有编码器和解码器，GPT2只有解码器，所以对于单向的文本生成任务特别适合。

#### 使用步骤

由于GPT2模型本质是训练一个语言模型，所以训练时用`textgen.language_generation.LanguageModelingModel`，
预测时用`textgen.language_modeling.LanguageGenerationModel`。

使用步骤：

1. 初始化 `LanguageModelingModel` 和对应的`tokenizer`
2. 训练模型 `train_model()`
3. 评估模型 `eval_model()`
4. 预测，调用`LanguageGenerationModel`的`generate`方法

### 训练及预测

查看 [examples/language_generation/GPT2_Finetune_Chinese_Couplet.ipynb](https://github.com/shibing624/textgen/blob/main/examples/language_generation/GPT2_Finetune_Chinese_Couplet.ipynb) ，里面包括训练和预测。

使用全量数据训练5个epochs。

### 结果预测

|input_text|target_text|pred|
|:--- |:--- |:-- |
|书香醉我凌云梦|诗韵酬余壮志心|情系流年醉海魂|

## 预测结果对比

|input_text|target_text|GPT2预测|T5预测|哪个模型效果好|
|:--- |:--- |:-- |:-- |:--:|
|书香醉我凌云梦|诗韵酬余壮志心|情系流年醉海魂|墨韵迷人醉月心|都差|
|春回大地，对对黄莺鸣暖树|日照神州，群群紫燕衔新泥|月下荷塘，笑笑白鸟唱青山|福至人间,家家紫燕舞和风|T5好|
|松风吼夜潮，巧剜明月染春水|小阁烹香茗，闲对茶经忆古人|水色染春风，明敲红线报春声|柳浪翻花浪,漫卷残阳湿画屏|都差|
|庙貌遍海澨山陬，虽僻处剡西，犹不废千秋俎豆|志业在平吴抑魏，只乃心王室，洵足扶万古纲常|春风沐园桃李绿，且欣时来时，且知时作一道菜肴|庙貌并云龙漳水,有灵山灵秀,更何分万古馨香|都差|
|磊落|光明|雄浑|清廉|都差|
|伤积在骨何依托|情结成茧怎生抽|难为人言自可言|痛饮于胸不动摇|都差|
|已彻骨深寒，倩影依稀，有我空庭长饮月|还伤情正浓，珠泪难拂，无君绣帘尽卷风|已醉诗酣睡，梅香未尽，无人半夜饮花|莫伤怀寂寞,愁容寂寞,无君旧梦又思乡|都差|
|紫气蒸茶山，芽鲜叶秀|清泉润竹海，笋嫩篁幽|香源涌水岭，绿浪涟涟|红霞映碧水,水秀山青|T5好|
|来生后世无因果|乱语胡言惹是非|生死后世有缘人|东倒西歪有缘由|T5好|
|雪盖梅梢，愿与红颜同白首|霞披芦顶，心仪鹤发度金婚|山边烟雾，听从绿叶逐清声|风梳柳辫,愿同碧水共青天|T5好|

分析：

1. 10个预测case，6个效果都差，说明中文古文的对联生成任务很难，两个模型无法准确理解上联的语义，生成的结果只是满足了字数相同、词性对齐、词面对齐，形似，
 语义对仗工整还差点儿，平仄合律也不满足。
2. 剩下4个case都是T5模型效果好些，说明双向的encoder-decoder网络结构（T5）比单向的decoder结构（GPT2）更适合对联生成任务。


## 展望

对联创作的基本要求是：平仄合律，对仗工整，文意切题。

三者相互依存，不可偏废。平仄合律是基础，不合律的对联就是不合格的对联。对仗工整是关键，
对仗不工整，即使平仄合律，也只能滥竽充数。文意切题是目的，平仄合律、对仗工整而文意不切题，就是无的放矢，甚至会产生相反的效果。

后续调研[SongNet](https://zhuanlan.zhihu.com/p/162216597)，该模型加入了模板和韵律特征，可以同时保证生成文本的格式正确、韵律合理、句子完整等基本的质量要求。

